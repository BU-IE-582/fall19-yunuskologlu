{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ancient times, competition is attractive to people. In ancient times arenas, in modern times the sport fields are the places where people can watch this competition.\n",
    "\n",
    "The main question is: Why do people like the competition?\n",
    "\n",
    "One answer is to predict the result of the competition. But this is not the root cause. The driving force behind is to make some financial gain while entertaining. In other words, combining work and entertainment together.\n",
    "\n",
    "In modern times, soccer is the most popular sport in all over the world.\n",
    "\n",
    "What is soccer?\n",
    "\n",
    "According Cambridge English Dictionary [1]:\n",
    "‘’Soccer is a game played between two teams of eleven people, where each team tries to win by kicking a ball into the other team's goal.’’\n",
    "\n",
    "As one can understand from this explanation, soccer is an easy game that people from all education level can understand and interpret it.\n",
    "\n",
    "There are three possible outcomes of this sport, the win of either teams and tie. According these outcomes, people combine the entertainment with financial gain and people see the financial gain as work. Therefore, soccer betting is a growing sector in the world.\n",
    "\n",
    "The next important question is: Are the results of soccer games predictable?\n",
    "\n",
    "Scientists for mathematics are interested in the answering question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A. Heuer, C. Müller and O. Rubner1[2] all physicists/chemists from the University of Münster in Germany, have analyzed soccer on a statistical level. As the scientists explain, a soccer match is equivalent to two teams throwing dice. Rolling a 6 means “goal,” and the number of attempts of both teams is fixed at the beginning of the match, reflecting each team’s fitness in that season. The higher its level of fitness, the more chances a team has to score a goal. How to determine each team’s fitness level is the main task of the scientists’ analysis. To do this, the researchers analyzed data from all soccer matches in the German Bundesliga between the 1977-’78 season and the 2007-’08 season (except for the 1991-’92 season). In this league, every team plays 34 matches per season.\n",
    " \n",
    "Based on the data, the scientists characterized team fitness as the goal difference within a game averaged over a season (in other words, the difference between number of goals scored and allowed by a team). The scientists’ analysis showed that goal difference is an even bigger influence on team fitness than the number of goals. In addition, based on previous results, the home advantage could be taken into account by a team-independent but season-dependent constant. Overall, the researchers found that a team’s fitness level remains constant throughout the season, although it changes between seasons.\n",
    "\n",
    "Using team fitness values, the scientists derived a formula to estimate the expected value of the goal difference in a particular match. The actual number of goals in a match (just like rolling dice) could be described as Poissonian processes; the events occur randomly and, for the most part, independently of each other. Taking all analyzed matches into account, the goal distribution determined in this way agrees almost perfectly with the actual data. \n",
    "\n",
    "“The three key results are (1) the observation of constant team fitness during a season, (2) the derivation of an equation which predicts the average outcome of a match, and (3) the observation that the actual goal distribution can be very well described by a Poisson distribution,” Heuer summarized.\n",
    "\n",
    "Although the researchers’ equation was accurate in many areas, the researchers found that it became less accurate in cases where the goal difference was one or zero. Specifically, in the real data, there were more zeros (ties) than predicted by the equation, and fewer one-goal differences. \n",
    "\n",
    "“The agreement with the actual data is perfect within statistical error if analyzing the goals per team,” Heuer said. “[However,] when analyzing the distribution of goal differences, one observes too many draws. This shows that the assumption of independent Poisson processes is not correct in cases where the goal difference is -1, 0, or 1. This points to interesting psychological effects, favoring a draw.”\n",
    "\n",
    "As the researchers note, there are other random effects that influence goals. These effects include temporary injuries, fatigue, weather conditions that favor one time over another, red cards, and so-called self-affirmative effects - that is, the probability of scoring a goal is increased when a team has already scored one or more goals in that game. Although the influence of these effects is very difficult to predict, the researchers found that these effects have a much smaller overall impact on the final outcome of a match as compared to the typical fitness differences. \n",
    "\n",
    "The analysis also has interesting effects on how we tend to view soccer matches, according to the researchers. For example, the media will often comment that a team that won or lost played particularly good or bad in that match. In contrast, the results here suggest that a team’s fitness level doesn’t change very much from match to match. Yet media reports (and fans) may have a strong tendency to judge a team’s fitness level based too much on the overall score, while ignoring the random effects that may have actually led to the overall score. \n",
    "\n",
    "B. Ulmer, M. Fernandez [3]   predict the results of soccer matches in the English Premier League (EPL) using artiﬁcial intelligence and machine learning algorithms. From historical data we created a feature set that includes gameday data and current team performance (form). Using our feature data they created ﬁve different classiﬁers: Linear from stochastic gradient descent, Naive Bayes, Hidden Markov Model, Support Vector Machine (SVM), and Random Forest. Our prediction is in one of three classes for each game: win, draw, or loss. Their best error rates were with our Linear classiﬁer (.48), Random Forest (.50), and SVM (.50). Their error analysis focused on improving hyperparameters and class imbalance, which we approached using grid searches and ROC curve analysis respectively.\n",
    "\n",
    "S. Sathe, D. Kasat, N. Kulkarni, R. Satao [4] indicate that Machine Learning allows to gain insight into data using which they aim to cover feature extraction for premier league football predictive analysis and perform machine learning to gain insight. The system will be performing their analysis based on our featured dataset and implement multiple classification algorithms such as support vector machine, random forest and naïve bayes. Best performing algorithm in their system was SVM having accuracy of 0.599 followed by Naïve Bayes of 0.55. They stated the accuracy can be further improved by adding more relevant features developing models which take into consider even broader aspects of football.\n",
    "\n",
    "[5] Nivard van Wijk, Based on data analysis of multiple seasons of the English Premier League he concludes that (1) there is a signiﬁcant home ground advantage, (2) the number of goals scored by the home and away team can be described by a Poisson distribution, (3) there are diﬀerences between the teams in the ability of winning points, and (4) the number of goals scored by the home team is correlated with the number of goals scored by the away team. Based on one or more of these conclusions, two types of models are formulated. Soccer has a very low scoring rate which makes it diﬃcult to diversify between a strong team (with an expected number of goals of 2) and a weak team (with an expected number of goals of 1).\n",
    "\n",
    "As in only 54% of the matches is predicted correctly. Because of the Poisson distributed number of goals, both teams will still have a reasonable probability of winning the match. \n",
    "\n",
    "In summary, \n",
    "•\tA soccer match is equivalent to two teams throwing dice.\n",
    "•\tThe fit of the team indicates that the fitter team has more chance to throw the dice.\n",
    "•\tThe fit of the team depends on certain features. To extract these features, data should be collected and interpreted.\n",
    "•\tThrowing a dice will result as a goal when the outcome is 6 or something like that \n",
    "•\tThe number of the goals are distributed Piossonly.\n",
    "•\tBecause of the Poisson distributed number of goals, both teams will still have a reasonable probability of winning the match.\n",
    "•\tSoccer has a very low scoring rate which makes it diﬃcult to diversify between a strong team (with an expected number of goals) and a weak team (with an expected number of goals).\n",
    "In this project, we aimed to make predictions for the results of several English Premier League matches using the statistics of previous matches and related odd values of the matches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, we used statistics of Premier League matches from 2005 to present. The data loaded from https://www.football-data.co.uk/englandm.php . The data has 148 different features for each match. However, we did not use these features directly since these statistics only relevant to a single match. In order to explain relations among matches, we linked infromation of related matches.(We did it by using object oriented program(ICRON)). At the end we came up with following features:\n",
    "\n",
    "Date:\t    Date of match\n",
    "\n",
    "HomeTeam:\tName of Team (Liverpool)\n",
    "\n",
    "AwayTeam:\tName of AwayTeam(Everton)\n",
    "\n",
    "HomeGoal:\tHome Team Score of the match\n",
    "\n",
    "AwayGoal:\tAway Team Score of the match\n",
    "\n",
    "Result: \t\tIf Home Team wins 1, if Away Team wins 2, 0 otherwise\t\n",
    "\n",
    "over/under:\t1 if total goal is more than 2.5,  0 otherwise\n",
    "\n",
    "PATAM_HW:\t# of Home win of last 5 matches of away team when away team doesn't play   its stadium. (Last 5 matches of Everton when Everton is away)\n",
    "\n",
    "PATAM_Tie\t# of Tie of last 5 matches of away team when away team doesn't play   its stadium. (Last 5 matches of Everton when Everton is away)\n",
    "\n",
    "PATAM_AW\t# of Away win - Last 5 matches of away team when away team doesn't play its stadium((Last 5 matches of Everton when Everton is away)\n",
    "\n",
    "PHTMH_HW\t# of Home win - Last 5 matches of home team when home team  play in its stadium (Last 5 matches of Liverpool when Liverpool is home team)\n",
    "\n",
    "PHTMH_Tie\t# of Tie - Last 5 matches of home team when home team  play in its stadium (Last 5 matches of Liverpool when Liverpool is home team)\n",
    "\n",
    "PHTMH_AW\t# of Away win - Last 5 matches of home team when home team  play in its stadium (Last 5 matches of Liverpool when Liverpool is home team)\n",
    "\n",
    "PM_HW\t# of Home win - Last 3 matches of teams played in the same stadium (Last 3 Liverpool-Everton matches played in Livepool’s stadium)\n",
    "\n",
    "PM_Tie\t\t# of Home win - Last 3 matches of teams played in the same stadium(Last 3 Liverpool-Everton matches played in Livepool’s stadium)\n",
    "\n",
    "PM_AW\t# of Home win - Last 3 matches of teams played in the same stadium(Last 3 Liverpool-Everton matches played in Livepool’s stadium)\n",
    "\n",
    "PHTM_HW\t# of Home win of last 5 matches of home team. (Last 5 matches of Liverpool regardless stadium)\n",
    "\n",
    "PHTM_Tie\t# of Tie of last 5 matches of home team. (Last 5 matches of Liverpool regardless stadium)\n",
    "\n",
    "PHTM_AW\t# of Away win of last 5 matches of home team. (Last 5 matches of Liverpool regardless stadium)\n",
    "\n",
    "PATM_HW\t# of Home win of last 5 matches of away team. (Last 5 matches of Everton regardless stadium)\n",
    "\n",
    "PATM_Tie\t# of Tie of last 5 matches of home team. (Last 5 matches of Everton regardless stadium)\n",
    "\n",
    "PATM_AW\t# of Away win of last 5 matches of home. (Last 5 matches of Everton regardless stadium)\n",
    "\n",
    "Norm_HW\tNormalized Home Win Probability of match based on odds\n",
    "\n",
    "Norm_Tie\tNormalized Tie Probability of match based on odds\n",
    "\n",
    "Norm_AW\tNormalized Away Win Probability of match based on odds\n",
    "\n",
    "Norm_over25\tNormalized over 2.5  Probability of match based on odds\n",
    "\n",
    "Norm_under25\tNormalized under 2.5  Probability of match based on odds\n",
    "\n",
    "HW_odd\tHome win odd of oddbookmaker\n",
    "\n",
    "Tie_odd\tTie odd of oddbookmaker\n",
    "\n",
    "AW_odd\tAway win odd of oddbookmaker\n",
    "\n",
    "over_odd\tover25 odd of oddbookmaker\n",
    "\n",
    "under_odd\tunder25 odd of oddbookmaker\n",
    "\n",
    "\n",
    "_______________________________________________________________________________________________________________________________\n",
    "\n",
    "Note: We mentioned details of data preparation in Homework 4. Since we used Phyton in implementation, we did not repeat same procedures. We used Excel data in Homework 4.\n",
    "\n",
    "At the beginning, we don’t want to use odds directly in our model since odds mostly dominate the other features. Although we spent significant effort, there were still %4 accuracy gap between model with odds and without odds. Therefore, we included odd averages of bookmakers in our model. \n",
    "\n",
    "In the second step, we normalized the features in order to avoid problems regarding scaling. However; we used raw data in some models, like decision trees, to interpret the results more easily.\n",
    "\n",
    "We normalized dependent features, such as away wins / ties / losses, and standardized the columns. \n",
    "\n",
    "Before running the learning methods, we applied feature selection process because most of the features could be highly correlated. In this process we used RFE function of Phyton. We obtained 5 different datasets by running SVC, logistic regression as RFE models and  factor analysis, PCA as the feature extraction models. \n",
    "\n",
    "The purpose of the project was minimizing RPS values, thus we used RPS as our test metric. For training metric, using RPS is illogical because of two reasons. Firstly, using RPS or accuracy would change nothing as the predictor would be forced to predict the probability as 1. Additionally, RPS is hard to take derivative.\n",
    "\n",
    "We evaluated decision tree, random forest, support vector machines, KNN, logistic regression learning methods. In order to get the best parameters, we used cross validation. Also, we run models for each data set that are mentioned above. Eventually, we selected the model with highest accuracy. It delivered the best RPS value as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import os\n",
    "import xlrd\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import make_regression\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import GaussianNoise\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import regularizers\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "rn.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "RANDOM_SEED = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\yunus\\\\Downloads')\n",
    "xls = pd.ExcelFile('Rikudou.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Liverpool = pd.read_excel('Rikudou.xlsx', sheet_name=\"Liverpool\")\n",
    "Leicester = pd.read_excel('Rikudou.xlsx', sheet_name=\"Leicester\")\n",
    "Mancity = pd.read_excel('Rikudou.xlsx', sheet_name=\"Man City\")\n",
    "Chelsea = pd.read_excel('Rikudou.xlsx', sheet_name=\"Chelsea\")\n",
    "Sheffield = pd.read_excel('Rikudou.xlsx', sheet_name=\"Sheffield\")\n",
    "Wolves = pd.read_excel('Rikudou.xlsx', sheet_name=\"Wolves\")\n",
    "Tottenham = pd.read_excel('Rikudou.xlsx', sheet_name=\"Tottenham\")\n",
    "Manu = pd.read_excel('Rikudou.xlsx', sheet_name=\"Man United\")\n",
    "Newcastle = pd.read_excel('Rikudou.xlsx', sheet_name=\"Newcastle\")\n",
    "Burnley = pd.read_excel('Rikudou.xlsx', sheet_name=\"Burnley\")\n",
    "Crystal = pd.read_excel('Rikudou.xlsx', sheet_name=\"Crystal Palace\")\n",
    "Bourne = pd.read_excel('Rikudou.xlsx', sheet_name=\"Bournemouth\")\n",
    "Everton = pd.read_excel('Rikudou.xlsx', sheet_name=\"Everton\")\n",
    "Southampton = pd.read_excel('Rikudou.xlsx', sheet_name=\"Southampton\")\n",
    "Norwich = pd.read_excel('Rikudou.xlsx', sheet_name=\"Norwich\")\n",
    "Watford = pd.read_excel('Rikudou.xlsx', sheet_name=\"Watford\")\n",
    "Astonvilla = pd.read_excel('Rikudou.xlsx', sheet_name=\"Aston Villa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with Liverpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Liverpool.iloc[:, 31].values\n",
    "x = Liverpool.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, by using Scikit-Learn Library, we split the data into test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all of the preprocessing steps are taken in Excel and SQL. For instance, take a look at feature set and the dependent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.50768717,  3.27347958, ...,  1.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 1.        ,  0.52972792, -1.09115986, ...,  1.        ,\n",
       "         1.        ,  1.        ],\n",
       "       [ 1.        ,  0.52972792, -1.09115986, ...,  0.        ,\n",
       "         1.        ,  1.        ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.93721094, -1.09115986, ...,  0.        ,\n",
       "         1.        ,  1.        ],\n",
       "       [ 1.        ,  0.52972792, -1.09115986, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.        ,  0.12224491,  1.5276238 , ...,  1.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 0, 1, 2, 2,\n",
       "       1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 0, 2, 1, 2, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 2, 0, 0, 2, 0, 1, 1, 2, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 2, 2, 1, 1, 2, 1, 0, 1, 2, 1, 1, 1,\n",
       "       0, 1, 2, 0, 2, 1, 1, 2, 0, 2, 2, 2, 1, 1, 2, 1, 2, 2, 0, 1, 1, 2,\n",
       "       1, 1, 0, 1, 2, 2, 1, 2, 1, 1, 0, 0, 1, 0, 1, 1, 2, 1, 0, 2, 2, 1,\n",
       "       2, 0, 1, 1, 0, 2, 2, 1, 0, 0, 0, 2, 1, 1, 2, 2, 1, 0, 0, 1, 2, 1,\n",
       "       0, 0, 1, 0, 1, 1, 2, 1, 0, 2, 0, 1, 1, 1, 2, 2, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 2, 0, 1, 1, 2, 1, 2, 2, 0, 2, 2, 2, 1, 2, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 2, 2, 1, 0, 2, 1, 0, 2, 2, 0, 1, 0, 0, 0, 1, 2, 1,\n",
       "       2, 2, 1, 2, 0, 2, 1, 2, 1, 1, 1, 2, 0, 1, 1, 0, 1, 0, 1, 2, 0, 1,\n",
       "       1, 0, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 2, 1, 1, 0, 1, 0, 2, 1, 0, 1,\n",
       "       0, 1, 1, 2, 0, 1, 0, 0, 2, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 2, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 2, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we first import RFE (Recursive Feature Elimination) with SVC (Support Vector Classifier) as our first feature engineering method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False, False,  True, False,  True, False,\n",
       "       False,  True,  True, False,  True,  True,  True, False, False,\n",
       "        True, False,  True,  True, False,  True, False, False,  True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "estimator = SVC(kernel=\"linear\", degree = 2)\n",
    "selector = RFE(estimator, 15, step=1)\n",
    "selector = selector.fit(x_train, y_train)\n",
    "selector.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = selector.support_\n",
    "b = []\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "while i < 27:\n",
    "  if a[i] == True:\n",
    "        b.append(i)\n",
    "  i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train = x_train[:, b]\n",
    "x1_test = x_test[:, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the same with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "selector = RFE(estimator, 15, step=1)\n",
    "selector = selector.fit(x_train, y_train)\n",
    "selector.support_\n",
    "a = selector.support_\n",
    "b = []\n",
    "i = 0\n",
    "while i < 27:\n",
    "  if a[i] == True:\n",
    "        b.append(i)\n",
    "  i += 1\n",
    "\n",
    "x2_train = x_train[:, b]\n",
    "x2_test = x_test[:, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "estimator = ExtraTreesClassifier()\n",
    "selector = RFE(estimator, 15, step=1)\n",
    "selector = selector.fit(x_train, y_train)\n",
    "selector.support_\n",
    "a = selector.support_\n",
    "b = []\n",
    "i = 0\n",
    "while i < 27:\n",
    "  if a[i] == True:\n",
    "        b.append(i)\n",
    "  i += 1\n",
    "\n",
    "x3_train = x_train[:, b]\n",
    "x3_test = x_test[:, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use Factor Analysis as a feature extraction method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5.02355364e+00,  3.21284921e+00,  2.29823783e+00,  2.13107594e+00,\n",
       "         1.89786601e+00,  1.79068017e+00,  1.63890350e+00,  1.52837091e+00,\n",
       "         1.27681688e+00,  1.05156820e+00,  8.20334055e-01,  8.00120335e-01,\n",
       "         6.92385291e-01,  6.38283023e-01,  5.83072992e-01,  4.82000340e-01,\n",
       "         3.60011942e-01,  3.49274812e-01,  2.63647335e-01,  1.60916815e-01,\n",
       "         2.20439775e-05,  4.96148149e-06,  2.95403038e-06,  8.03815472e-07,\n",
       "         4.52355438e-12,  3.33213479e-16, -3.23043267e-16]),\n",
       " array([ 4.63544501,  2.67903915,  1.96762827,  1.3421832 ,  1.12754271,\n",
       "         0.93682527,  0.83291631,  0.67272034,  0.50145244,  0.30686872,\n",
       "         0.15053704,  0.12330113,  0.09221198, -0.04977053, -0.10274528,\n",
       "        -0.15484569, -0.18688297, -0.23962783, -0.26523392, -0.32909818,\n",
       "        -0.39928732, -0.41583272, -0.56622935, -0.69520188, -0.71671704,\n",
       "        -0.94116921, -0.95583152]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "fa = FactorAnalyzer()\n",
    "fa.fit(x_train, 23)\n",
    "fa.get_eigenvalues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the factors, we will get the factors which have their eigenvalues higher than or close to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = FactorAnalyzer(n_factors= 7, rotation = None)\n",
    "x4_train = transformer.fit_transform(x_train)\n",
    "x4_test = transformer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will apply PCA as our feature extraction method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18829507, 0.13696557, 0.10303266, 0.10218903, 0.08344369,\n",
       "       0.07221164, 0.0678592 , 0.04970704, 0.0369736 , 0.03120989,\n",
       "       0.02890755, 0.02260643, 0.01833562, 0.01710436, 0.01288106])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=15, svd_solver='arpack')\n",
    "pca.fit(x_train)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get first 12 Principal Components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=12, svd_solver='arpack')\n",
    "x5_train = pca.fit_transform(x_train)\n",
    "x5_test = pca.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to apply machine learning models on these dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with K-NN. We will use 10-fold Grid Search in order to optimize Hyperparameters in the models that require Hyperparameters. We start with Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RPS(predictions, observed):\n",
    "   ncat = 3\n",
    "   npred = len(predictions)\n",
    "   RPS = np.zeros(npred)\n",
    "   for x in range(0, npred):\n",
    "      obsvec = np.zeros(ncat)\n",
    "      obsvec[observed[x]-1] = 1\n",
    "      cumulative = 0\n",
    "      for i in range(1, ncat):\n",
    "          cumulative = cumulative + (sum(predictions[x, 1:i]) - sum(obsvec[1:i])) ** 2\n",
    "          RPS[x] = (1/(ncat-1)) * cumulative\n",
    "   return RPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Liverpool_Logistic_0_Rps = statistics.mean(RPS(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1901141915202747"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Liverpool_Logistic_0_Rps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression provides an RPS score of 0.19 for Liverpool, for first dataset. Now we will apply the same methodology for other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19389033448025608"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x1_train, y_train)\n",
    "y1_pred = model.predict_proba(x1_test)\n",
    "Liverpool_Logistic_1_Rps = statistics.mean(RPS(y1_pred, y_test))\n",
    "Liverpool_Logistic_1_Rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19228505481573657"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x2_train, y_train)\n",
    "y2_pred = model.predict_proba(x2_test)\n",
    "Liverpool_Logistic_2_Rps = statistics.mean(RPS(y2_pred, y_test))\n",
    "Liverpool_Logistic_2_Rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17918763125112228"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x3_train, y_train)\n",
    "y3_pred = model.predict_proba(x3_test)\n",
    "Liverpool_Logistic_3_Rps = statistics.mean(RPS(y3_pred, y_test))\n",
    "Liverpool_Logistic_3_Rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10447014370864845"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x4_train, y_train)\n",
    "y4_pred = model.predict_proba(x4_test)\n",
    "Liverpool_Logistic_4_Rps = statistics.mean(RPS(y4_pred, y_test))\n",
    "Liverpool_Logistic_4_Rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18715474930862133"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x5_train, y_train)\n",
    "y5_pred = model.predict_proba(x5_test)\n",
    "Liverpool_Logistic_5_Rps = statistics.mean(RPS(y5_pred, y_test))\n",
    "Liverpool_Logistic_5_Rps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, Logistic Regression reaches 0.1 RPS score for Liverpool. Now, we continue with other methods for Liverpool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with SVC. We apply Grid search first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C':[1,10,100],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf', 'poly']}\n",
    "grid = GridSearchCV(SVC(),param_grid,refit = True, verbose=0)\n",
    "grid.fit(x_train,y_train)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal parameters are C = 100, gamma = 0.001, kernel = rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12986713325461025"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(C = 100, gamma = 0.001, kernel = 'rbf', probability = True)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict_proba(x_test)\n",
    "Liverpool_SVC_0_Rps = statistics.mean(RPS(y_pred, y_test))\n",
    "Liverpool_SVC_0_Rps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial dataset obtains 0.13 for Liverpool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 1, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C':[1,10,100],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf', 'poly']}\n",
    "grid = GridSearchCV(SVC(),param_grid,refit = True, verbose=0)\n",
    "grid.fit(x1_train,y_train)\n",
    "\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13746534850359635"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(C = 10, gamma = 1, kernel = 'linear', probability = True)\n",
    "model.fit(x1_train, y_train)\n",
    "y1_pred = model.predict_proba(x1_test)\n",
    "Liverpool_SVC_1_Rps = statistics.mean(RPS(y1_pred, y_test))\n",
    "Liverpool_SVC_1_Rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C':[1,10,100],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf', 'poly']}\n",
    "grid = GridSearchCV(SVC(),param_grid,refit = True, verbose=0)\n",
    "grid.fit(x2_train,y_train)\n",
    "\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14588792879768656"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(C = 10, gamma = 1, kernel = 'linear', probability = True)\n",
    "model.fit(x2_train, y_train)\n",
    "y2_pred = model.predict_proba(x2_test)\n",
    "Liverpool_SVC_2_Rps = statistics.mean(RPS(y2_pred, y_test))\n",
    "Liverpool_SVC_2_Rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C':[1,10,100],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf', 'poly']}\n",
    "grid = GridSearchCV(SVC(),param_grid,refit = True, verbose=0)\n",
    "grid.fit(x3_train,y_train)\n",
    "\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13818755803059998"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(C = 100, gamma = 0.001, kernel = 'rbf', probability = True)\n",
    "model.fit(x3_train, y_train)\n",
    "y3_pred = model.predict_proba(x3_test)\n",
    "Liverpool_SVC_3_Rps = statistics.mean(RPS(y3_pred, y_test))\n",
    "Liverpool_SVC_3_Rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11866485679732447"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(C = 100, gamma = 0.001, kernel = 'rbf', probability = True)\n",
    "model.fit(x4_train, y_train)\n",
    "y4_pred = model.predict_proba(x4_test)\n",
    "Liverpool_SVC_4_Rps = statistics.mean(RPS(y4_pred, y_test))\n",
    "Liverpool_SVC_4_Rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 1, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C':[1,10,100],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf', 'poly']}\n",
    "grid = GridSearchCV(SVC(),param_grid,refit = True, verbose=0)\n",
    "grid.fit(x5_train,y_train)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1282022469539"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(C = 1, gamma = 1, kernel = 'linear', probability = True)\n",
    "model.fit(x5_train, y_train)\n",
    "y5_pred = model.predict_proba(x5_test)\n",
    "Liverpool_SVC_5_Rps = statistics.mean(RPS(y5_pred, y_test))\n",
    "Liverpool_SVC_5_Rps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best RPS is obtained from Dataset 4, with C = 100, Gamma = 0.001 and RBF kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metric': 'manhattan', 'n_neighbors': 17}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "param_grid = {'n_neighbors': np.arange(1, 20), 'metric': [\"euclidean\", \"manhattan\"] }\n",
    "knn = KNeighborsClassifier()\n",
    "knn_gscv = GridSearchCV(knn, param_grid, cv=10)\n",
    "knn_gscv.fit(x_train, y_train)\n",
    "knn_gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16636577904819216"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors = 17, metric = \"manhattan\")\n",
    "knn_model.fit(x_train, y_train)\n",
    "y_pred = knn_model.predict_proba(x_test)\n",
    "Liverpool_KNN_0_Rps = statistics.mean(RPS(y_pred, y_test))\n",
    "Liverpool_KNN_0_Rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metric': 'manhattan', 'n_neighbors': 17}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'n_neighbors': np.arange(1, 20), 'metric': [\"euclidean\", \"manhattan\"] }\n",
    "knn = KNeighborsClassifier()\n",
    "knn_gscv = GridSearchCV(knn, param_grid, cv=10)\n",
    "knn_gscv.fit(x1_train, y_train)\n",
    "knn_gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13746534850359635"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors = 17, metric = \"manhattan\")\n",
    "knn_model.fit(x1_train, y_train)\n",
    "y_pred = knn_model.predict_proba(x1_test)\n",
    "Liverpool_KNN_1_Rps = statistics.mean(RPS(y1_pred, y_test))\n",
    "Liverpool_KNN_1_Rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metric': 'manhattan', 'n_neighbors': 16}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'n_neighbors': np.arange(1, 20), 'metric': [\"euclidean\", \"manhattan\"] }\n",
    "knn = KNeighborsClassifier()\n",
    "knn_gscv = GridSearchCV(knn, param_grid, cv=10)\n",
    "knn_gscv.fit(x2_train, y_train)\n",
    "knn_gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16887454710144928"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors = 16, metric = \"manhattan\")\n",
    "knn_model.fit(x2_train, y_train)\n",
    "y2_pred = knn_model.predict_proba(x2_test)\n",
    "Liverpool_KNN_2_Rps = statistics.mean(RPS(y2_pred, y_test))\n",
    "Liverpool_KNN_2_Rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metric': 'euclidean', 'n_neighbors': 12}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'n_neighbors': np.arange(1, 20), 'metric': [\"euclidean\", \"manhattan\"] }\n",
    "knn = KNeighborsClassifier()\n",
    "knn_gscv = GridSearchCV(knn, param_grid, cv=10)\n",
    "knn_gscv.fit(x3_train, y_train)\n",
    "knn_gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17373752570081744"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors = 17, metric = \"manhattan\")\n",
    "knn_model.fit(x3_train, y_train)\n",
    "y3_pred = knn_model.predict_proba(x3_test)\n",
    "Liverpool_KNN_3_Rps = statistics.mean(RPS(y3_pred, y_test))\n",
    "Liverpool_KNN_3_Rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metric': 'euclidean', 'n_neighbors': 13}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'n_neighbors': np.arange(1, 20), 'metric': [\"euclidean\", \"manhattan\"] }\n",
    "knn = KNeighborsClassifier()\n",
    "knn_gscv = GridSearchCV(knn, param_grid, cv=10)\n",
    "knn_gscv.fit(x4_train, y_train)\n",
    "knn_gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1232741617357002"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors = 13, metric = \"euclidean\")\n",
    "knn_model.fit(x4_train, y_train)\n",
    "y4_pred = knn_model.predict_proba(x4_test)\n",
    "Liverpool_KNN_4_Rps = statistics.mean(RPS(y4_pred, y_test))\n",
    "Liverpool_KNN_4_Rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metric': 'manhattan', 'n_neighbors': 19}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'n_neighbors': np.arange(1, 20), 'metric': [\"euclidean\", \"manhattan\"] }\n",
    "knn = KNeighborsClassifier()\n",
    "knn_gscv = GridSearchCV(knn, param_grid, cv=10)\n",
    "knn_gscv.fit(x5_train, y_train)\n",
    "knn_gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1591994861295114"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors = 19, metric = \"manhattan\")\n",
    "knn_model.fit(x5_train, y_train)\n",
    "y5_pred = knn_model.predict_proba(x5_test)\n",
    "Liverpool_KNN_5_Rps = statistics.mean(RPS(y5_pred, y_test))\n",
    "Liverpool_KNN_5_Rps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally finished K-NN. Best RPS is obtained by Factor Analysis dataset, by using 13 neighbours and euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Liverpool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.190114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.193890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.192285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.179188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.104470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.187155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.129867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.137465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.145888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.138188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.118665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.128202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.166366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.137465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.168875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.173738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.123274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.159199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Liverpool\n",
       "0    0.190114\n",
       "1    0.193890\n",
       "2    0.192285\n",
       "3    0.179188\n",
       "4    0.104470\n",
       "5    0.187155\n",
       "6    0.129867\n",
       "7    0.137465\n",
       "8    0.145888\n",
       "9    0.138188\n",
       "10   0.118665\n",
       "11   0.128202\n",
       "12   0.166366\n",
       "13   0.137465\n",
       "14   0.168875\n",
       "15   0.173738\n",
       "16   0.123274\n",
       "17   0.159199"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Liverpool_Data = {'Liverpool':[Liverpool_Logistic_0_Rps, Liverpool_Logistic_1_Rps, Liverpool_Logistic_2_Rps, Liverpool_Logistic_3_Rps,\n",
    "                     Liverpool_Logistic_4_Rps, Liverpool_Logistic_5_Rps, Liverpool_SVC_0_Rps, Liverpool_SVC_1_Rps,\n",
    "                     Liverpool_SVC_2_Rps, Liverpool_SVC_3_Rps, Liverpool_SVC_4_Rps, Liverpool_SVC_5_Rps, Liverpool_KNN_0_Rps,\n",
    "                     Liverpool_KNN_1_Rps, Liverpool_KNN_2_Rps, Liverpool_KNN_3_Rps, Liverpool_KNN_4_Rps, Liverpool_KNN_5_Rps]} \n",
    "Liverpool_Df = pd.DataFrame(Liverpool_Data) \n",
    "Liverpool_Df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the we will repeat the same steps for other 19 teams. But first, we must make that a formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baydogan(x,y):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)\n",
    "    estimator = SVC(kernel=\"linear\", degree = 2)\n",
    "    selector = RFE(estimator, 15, step=1)\n",
    "    selector = selector.fit(x_train, y_train)\n",
    "    a = selector.support_\n",
    "    b = []\n",
    "    i = 0\n",
    "    while i < 27:\n",
    "      if a[i] == True:\n",
    "        b.append(i)\n",
    "      i += 1\n",
    "    x1_train = x_train[:, b]\n",
    "    x1_test = x_test[:, b]\n",
    "    estimator = LogisticRegression()\n",
    "    selector = RFE(estimator, 15, step=1)\n",
    "    selector = selector.fit(x_train, y_train)\n",
    "    a = selector.support_\n",
    "    b = []\n",
    "    i = 0\n",
    "    while i < 27:\n",
    "      if a[i] == True:\n",
    "        b.append(i)\n",
    "      i += 1\n",
    "\n",
    "    x2_train = x_train[:, b]\n",
    "    x2_test = x_test[:, b]\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    estimator = ExtraTreesClassifier()\n",
    "    selector = RFE(estimator, 15, step=1)\n",
    "    selector = selector.fit(x_train, y_train)\n",
    "    a = selector.support_\n",
    "    b = []\n",
    "    i = 0\n",
    "    while i < 27:\n",
    "      if a[i] == True:\n",
    "        b.append(i)\n",
    "      i += 1\n",
    "\n",
    "    x3_train = x_train[:, b]\n",
    "    x3_test = x_test[:, b]\n",
    "\n",
    "    from factor_analyzer import FactorAnalyzer\n",
    "    fa = FactorAnalyzer()\n",
    "    fa.fit(x_train, 23)\n",
    "    a = fa.get_eigenvalues()\n",
    "    a1 = a[1:2]\n",
    "    a2 =  list(a1)\n",
    "    a3 = a2[0]\n",
    "    b = 0\n",
    "    i = 0\n",
    "    while i < 27:\n",
    "      if a3[i] >= 0:\n",
    "            b += 1\n",
    "      i += 1\n",
    "    transformer = FactorAnalyzer(n_factors= b, rotation = None)\n",
    "    x4_train = transformer.fit_transform(x_train)\n",
    "    x4_test = transformer.transform(x_test)\n",
    "    pca = PCA(n_components=20, svd_solver='arpack')\n",
    "    pca.fit(x_train)\n",
    "    a = pca.explained_variance_ratio_\n",
    "    b = 0\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < 27:\n",
    "      if j <= 0.8 :\n",
    "            j = j + a[i]\n",
    "            b += 1\n",
    "      i += 1\n",
    "\n",
    "    pca = PCA(n_components=b, svd_solver='arpack')\n",
    "    x5_train = pca.fit_transform(x_train)\n",
    "    x5_test = pca.transform(x_test)\n",
    "    model = LogisticRegression(solver = 'lbfgs')\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict_proba(x_test)\n",
    "    Leicester_Logistic_0_Rps = statistics.mean(RPS(y_pred, y_test))\n",
    "    model = LogisticRegression(solver = 'lbfgs')\n",
    "    model.fit(x1_train, y_train)\n",
    "    y1_pred = model.predict_proba(x1_test)\n",
    "    Leicester_Logistic_1_Rps = statistics.mean(RPS(y1_pred, y_test))\n",
    "    Leicester_Logistic_1_Rps\n",
    "    model = LogisticRegression(solver = 'lbfgs')\n",
    "    model.fit(x2_train, y_train)\n",
    "    y2_pred = model.predict_proba(x2_test)\n",
    "    Leicester_Logistic_2_Rps = statistics.mean(RPS(y2_pred, y_test))\n",
    "    Leicester_Logistic_2_Rps\n",
    "    model = LogisticRegression(solver = 'lbfgs')\n",
    "    model.fit(x3_train, y_train)\n",
    "    y3_pred = model.predict_proba(x3_test)\n",
    "    Leicester_Logistic_3_Rps = statistics.mean(RPS(y3_pred, y_test))\n",
    "    Leicester_Logistic_3_Rps\n",
    "    model = LogisticRegression(solver = 'lbfgs')\n",
    "    model.fit(x4_train, y_train)\n",
    "    y4_pred = model.predict_proba(x4_test)\n",
    "    Leicester_Logistic_4_Rps = statistics.mean(RPS(y4_pred, y_test))\n",
    "    Leicester_Logistic_4_Rps\n",
    "    model = LogisticRegression(solver = 'lbfgs')\n",
    "    model.fit(x5_train, y_train)\n",
    "    y5_pred = model.predict_proba(x5_test)\n",
    "    Leicester_Logistic_5_Rps = statistics.mean(RPS(y5_pred, y_test))\n",
    "    Leicester_Logistic_5_Rps\n",
    "    param_grid = {'C':[1,10,100],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf', 'poly']}\n",
    "    grid = GridSearchCV(SVC(),param_grid,refit = True, verbose=0)\n",
    "    grid.fit(x_train,y_train)\n",
    "    a = grid.best_params_\n",
    "    model = SVC(C = a[\"C\"], gamma = a[\"gamma\"], kernel = a[\"kernel\"], probability = True)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict_proba(x_test)\n",
    "    Leicester_SVC_0_Rps = statistics.mean(RPS(y_pred, y_test))\n",
    "    grid.fit(x1_train,y_train)\n",
    "    a = grid.best_params_\n",
    "    model = SVC(C = a[\"C\"], gamma = a[\"gamma\"], kernel = a[\"kernel\"], probability = True)\n",
    "    model.fit(x1_train, y_train)\n",
    "    y1_pred = model.predict_proba(x1_test)\n",
    "    Leicester_SVC_1_Rps = statistics.mean(RPS(y1_pred, y_test))\n",
    "    grid.fit(x2_train,y_train)\n",
    "    a = grid.best_params_\n",
    "    model = SVC(C = a[\"C\"], gamma = a[\"gamma\"], kernel = a[\"kernel\"], probability = True)\n",
    "    model.fit(x2_train, y_train)\n",
    "    y2_pred = model.predict_proba(x2_test)\n",
    "    Leicester_SVC_2_Rps = statistics.mean(RPS(y2_pred, y_test))\n",
    "    model = SVC(C = a[\"C\"], gamma = a[\"gamma\"], kernel = a[\"kernel\"], probability = True)\n",
    "    model.fit(x3_train, y_train)\n",
    "    y3_pred = model.predict_proba(x3_test)\n",
    "    Leicester_SVC_3_Rps = statistics.mean(RPS(y3_pred, y_test))\n",
    "    model = SVC(C = a[\"C\"], gamma = a[\"gamma\"], kernel = a[\"kernel\"], probability = True)\n",
    "    model.fit(x4_train, y_train)\n",
    "    y4_pred = model.predict_proba(x4_test)\n",
    "    Leicester_SVC_4_Rps = statistics.mean(RPS(y4_pred, y_test))\n",
    "    grid.fit(x5_train,y_train)\n",
    "    a = grid.best_params_\n",
    "    model = SVC(C = a[\"C\"], gamma = a[\"gamma\"], kernel = a[\"kernel\"], probability = True)\n",
    "    model.fit(x5_train, y_train)\n",
    "    y5_pred = model.predict_proba(x5_test)\n",
    "    Leicester_SVC_5_Rps = statistics.mean(RPS(y5_pred, y_test))\n",
    "    param_grid = {'n_neighbors': np.arange(1, 20), 'metric': [\"euclidean\", \"manhattan\"] }\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn_gscv = GridSearchCV(knn, param_grid, cv=10)\n",
    "    knn_gscv.fit(x_train, y_train)\n",
    "    a = knn_gscv.best_params_\n",
    "    knn_model = KNeighborsClassifier(n_neighbors = a[\"n_neighbors\"], metric = a[\"metric\"] )\n",
    "    knn_model.fit(x_train, y_train)\n",
    "    y_pred = knn_model.predict_proba(x_test)\n",
    "    Leicester_KNN_0_Rps = statistics.mean(RPS(y_pred, y_test))\n",
    "    knn_gscv.fit(x1_train, y_train)\n",
    "    a = knn_gscv.best_params_\n",
    "    knn_model = KNeighborsClassifier(n_neighbors = a[\"n_neighbors\"], metric = a[\"metric\"] )\n",
    "    knn_model.fit(x1_train, y_train)\n",
    "    y1_pred = knn_model.predict_proba(x1_test)\n",
    "    Leicester_KNN_1_Rps = statistics.mean(RPS(y1_pred, y_test))\n",
    "    knn_gscv.fit(x2_train, y_train)\n",
    "    a = knn_gscv.best_params_\n",
    "    knn_model = KNeighborsClassifier(n_neighbors = a[\"n_neighbors\"], metric = a[\"metric\"] )\n",
    "    knn_model.fit(x2_train, y_train)\n",
    "    y2_pred = knn_model.predict_proba(x2_test)\n",
    "    Leicester_KNN_2_Rps = statistics.mean(RPS(y2_pred, y_test))\n",
    "    knn_gscv.fit(x3_train, y_train)\n",
    "    a = knn_gscv.best_params_\n",
    "    knn_model = KNeighborsClassifier(n_neighbors = a[\"n_neighbors\"], metric = a[\"metric\"] )\n",
    "    knn_model.fit(x3_train, y_train)\n",
    "    y3_pred = knn_model.predict_proba(x3_test)\n",
    "    Leicester_KNN_3_Rps = statistics.mean(RPS(y3_pred, y_test))\n",
    "    knn_gscv.fit(x4_train, y_train)\n",
    "    a = knn_gscv.best_params_\n",
    "    knn_model = KNeighborsClassifier(n_neighbors = a[\"n_neighbors\"], metric = a[\"metric\"] )\n",
    "    knn_model.fit(x4_train, y_train)\n",
    "    y4_pred = knn_model.predict_proba(x4_test)\n",
    "    Leicester_KNN_4_Rps = statistics.mean(RPS(y4_pred, y_test))\n",
    "    knn_gscv.fit(x5_train, y_train)\n",
    "    a = knn_gscv.best_params_\n",
    "    knn_model = KNeighborsClassifier(n_neighbors = a[\"n_neighbors\"], metric = a[\"metric\"] )\n",
    "    knn_model.fit(x5_train, y_train)\n",
    "    y5_pred = knn_model.predict_proba(x5_test)\n",
    "    Leicester_KNN_5_Rps = statistics.mean(RPS(y5_pred, y_test))\n",
    "    Leicester_Data = {'Leicester':[Leicester_Logistic_0_Rps, Leicester_Logistic_1_Rps, Leicester_Logistic_2_Rps, Leicester_Logistic_3_Rps,\n",
    "                     Leicester_Logistic_4_Rps, Leicester_Logistic_5_Rps, Leicester_SVC_0_Rps, Leicester_SVC_1_Rps,\n",
    "                     Leicester_SVC_2_Rps, Leicester_SVC_3_Rps, Leicester_SVC_4_Rps, Leicester_SVC_5_Rps, Leicester_KNN_0_Rps,\n",
    "                     Leicester_KNN_1_Rps, Leicester_KNN_2_Rps, Leicester_KNN_3_Rps, Leicester_KNN_4_Rps, Leicester_KNN_5_Rps]} \n",
    "    Leicester_Df = pd.DataFrame(Leicester_Data)\n",
    "    return Leicester_Df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Leicester.iloc[:, 31].values\n",
    "x = Leicester.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Leicester_models = baydogan(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Mancity.iloc[:, 31].values\n",
    "x = Mancity.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Mancity_models = baydogan(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Chelsea.iloc[:, 31].values\n",
    "x = Chelsea.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Chelsea_models = baydogan(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Sheffield.iloc[:, 31].values\n",
    "x = Sheffield.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Sheffield_models = baydogan(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Wolves.iloc[:, 31].values\n",
    "x = Wolves.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Wolves_models = baydogan(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Tottenham.iloc[:, 31].values\n",
    "x = Tottenham.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Tottenham_models = baydogan(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Manu.iloc[:, 31].values\n",
    "x = Manu.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Manu = baydogan(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Newcastle.iloc[:, 31].values\n",
    "x = Newcastle.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Newcastle_models = baydogan(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Burnley.iloc[:, 31].values\n",
    "x = Burnley.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Burnley = baydogan(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Crystal.iloc[:, 31].values\n",
    "x = Crystal.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Crystal = baydogan(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Bourne.iloc[:, 31].values\n",
    "x = Bourne.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Bourne = baydogan(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Everton.iloc[:, 31].values\n",
    "x = Everton.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Everton = baydogan(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Southampton.iloc[:, 31].values\n",
    "x = Southampton.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Southampton = baydogan(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Norwich.iloc[:, 31].values\n",
    "x = Norwich.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Norwich_models = baydogan(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Watford.iloc[:, 31].values\n",
    "x = Watford.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Watford_models = baydogan(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Astonvilla.iloc[:, 31].values\n",
    "x = Astonvilla.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Astonvilla_models = baydogan(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arsenal = pd.read_excel('Rikudou.xlsx', sheet_name=\"Arsenal\")\n",
    "y = Arsenal.iloc[:, 31].values\n",
    "x = Arsenal.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "Arsenal_models = baydogan(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the lack of past info, Westham and Brighton fails in these methods. Thus, we will only use the previously mentioned methods without parameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brighton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.252131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.246888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.243910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.230699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.152043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.210697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.190381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.193337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.208733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.181849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.166737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.173909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.174687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.195156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.181406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.176094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.136328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.184609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Brighton\n",
       "0   0.252131\n",
       "1   0.246888\n",
       "2   0.243910\n",
       "3   0.230699\n",
       "4   0.152043\n",
       "5   0.210697\n",
       "6   0.190381\n",
       "7   0.193337\n",
       "8   0.208733\n",
       "9   0.181849\n",
       "10  0.166737\n",
       "11  0.173909\n",
       "12  0.174687\n",
       "13  0.195156\n",
       "14  0.181406\n",
       "15  0.176094\n",
       "16  0.136328\n",
       "17  0.184609"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Brighton = pd.read_excel('Rikudou.xlsx', sheet_name=\"Brighton\")\n",
    "y = Brighton.iloc[:, 31].values\n",
    "x = Brighton.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)\n",
    "estimator = SVC(kernel=\"linear\", degree = 2)\n",
    "selector = RFE(estimator, 15, step=1)\n",
    "selector = selector.fit(x_train, y_train)\n",
    "a = selector.support_\n",
    "b = []\n",
    "i = 0\n",
    "while i < 27:\n",
    "    if a[i] == True:\n",
    "        b.append(i)\n",
    "    i += 1\n",
    "x1_train = x_train[:, b]\n",
    "x1_test = x_test[:, b]\n",
    "estimator = LogisticRegression()\n",
    "selector = RFE(estimator, 15, step=1)\n",
    "selector = selector.fit(x_train, y_train)\n",
    "a = selector.support_\n",
    "b = []\n",
    "i = 0\n",
    "while i < 27:\n",
    "    if a[i] == True:\n",
    "        b.append(i)\n",
    "    i += 1\n",
    "x2_train = x_train[:, b]\n",
    "x2_test = x_test[:, b]\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "estimator = ExtraTreesClassifier()\n",
    "selector = RFE(estimator, 15, step=1)\n",
    "selector = selector.fit(x_train, y_train)\n",
    "a = selector.support_\n",
    "b = []\n",
    "i = 0\n",
    "while i < 27:\n",
    "    if a[i] == True:\n",
    "        b.append(i)\n",
    "    i += 1\n",
    "x3_train = x_train[:, b]\n",
    "x3_test = x_test[:, b]\n",
    "fa = FactorAnalyzer()\n",
    "fa.fit(x_train, 23)\n",
    "a = fa.get_eigenvalues()\n",
    "a1 = a[1:2]\n",
    "a2 =  list(a1)\n",
    "a3 = a2[0]\n",
    "b = 0\n",
    "i = 0\n",
    "while i < 27:\n",
    "    if a3[i] >= 0:\n",
    "        b += 1\n",
    "    i += 1\n",
    "transformer = FactorAnalyzer(n_factors= b, rotation = None)\n",
    "x4_train = transformer.fit_transform(x_train)\n",
    "x4_test = transformer.transform(x_test)\n",
    "pca = PCA(n_components=20, svd_solver='arpack')\n",
    "pca.fit(x_train)\n",
    "a = pca.explained_variance_ratio_\n",
    "b = 0\n",
    "i = 0\n",
    "j = 0\n",
    "while i < 27:\n",
    "    if j <= 0.8 :\n",
    "        j = j + a[i]\n",
    "        b += 1\n",
    "    i += 1\n",
    "\n",
    "pca = PCA(n_components=b, svd_solver='arpack')\n",
    "x5_train = pca.fit_transform(x_train)\n",
    "x5_test = pca.transform(x_test)\n",
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict_proba(x_test)\n",
    "Brighton_Logistic_0_Rps = statistics.mean(RPS(y_pred, y_test))\n",
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x1_train, y_train)\n",
    "y1_pred = model.predict_proba(x1_test)\n",
    "Brighton_Logistic_1_Rps = statistics.mean(RPS(y1_pred, y_test))\n",
    "Brighton_Logistic_1_Rps\n",
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x2_train, y_train)\n",
    "y2_pred = model.predict_proba(x2_test)\n",
    "Brighton_Logistic_2_Rps = statistics.mean(RPS(y2_pred, y_test))\n",
    "Brighton_Logistic_2_Rps\n",
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x3_train, y_train)\n",
    "y3_pred = model.predict_proba(x3_test)\n",
    "Brighton_Logistic_3_Rps = statistics.mean(RPS(y3_pred, y_test))\n",
    "Brighton_Logistic_3_Rps\n",
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x4_train, y_train)\n",
    "y4_pred = model.predict_proba(x4_test)\n",
    "Brighton_Logistic_4_Rps = statistics.mean(RPS(y4_pred, y_test))\n",
    "Brighton_Logistic_4_Rps\n",
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x5_train, y_train)\n",
    "y5_pred = model.predict_proba(x5_test)\n",
    "Brighton_Logistic_5_Rps = statistics.mean(RPS(y5_pred, y_test))\n",
    "Brighton_Logistic_5_Rps\n",
    "param_grid = {'C':[1,10,100],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf', 'poly']}\n",
    "grid = GridSearchCV(SVC(),param_grid,refit = True, verbose=0)\n",
    "grid.fit(x_train,y_train)\n",
    "a = grid.best_params_\n",
    "model = SVC(C = 10, gamma = 0.1, kernel = 'rbf', probability = True)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict_proba(x_test)\n",
    "Brighton_SVC_0_Rps = statistics.mean(RPS(y_pred, y_test))\n",
    "grid.fit(x1_train,y_train)\n",
    "a = grid.best_params_\n",
    "model = SVC(C = 10, gamma = 0.1, kernel = 'rbf', probability = True)\n",
    "model.fit(x1_train, y_train)\n",
    "y1_pred = model.predict_proba(x1_test)\n",
    "Brighton_SVC_1_Rps = statistics.mean(RPS(y1_pred, y_test))\n",
    "grid.fit(x2_train,y_train)\n",
    "a = grid.best_params_\n",
    "model = SVC(C = 10, gamma = 0.1, kernel = 'rbf', probability = True)\n",
    "model.fit(x2_train, y_train)\n",
    "y2_pred = model.predict_proba(x2_test)\n",
    "Brighton_SVC_2_Rps = statistics.mean(RPS(y2_pred, y_test))\n",
    "model = SVC(C = 10, gamma = 0.1, kernel = 'rbf', probability = True)\n",
    "model.fit(x3_train, y_train)\n",
    "y3_pred = model.predict_proba(x3_test)\n",
    "Brighton_SVC_3_Rps = statistics.mean(RPS(y3_pred, y_test))\n",
    "model = SVC(C = 10, gamma = 0.1, kernel = 'rbf', probability = True)\n",
    "model.fit(x4_train, y_train)\n",
    "y4_pred = model.predict_proba(x4_test)\n",
    "Brighton_SVC_4_Rps = statistics.mean(RPS(y4_pred, y_test))\n",
    "model = SVC(C = 10, gamma = 0.1, kernel = 'rbf', probability = True)\n",
    "model.fit(x5_train, y_train)\n",
    "y5_pred = model.predict_proba(x5_test)\n",
    "Brighton_SVC_5_Rps = statistics.mean(RPS(y5_pred, y_test))\n",
    "param_grid = {'n_neighbors': np.arange(1, 20), 'metric': [\"euclidean\", \"manhattan\"] }\n",
    "knn = KNeighborsClassifier()\n",
    "a = knn_gscv.best_params_\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 20, metric = \"euclidean\")\n",
    "knn_model.fit(x_train, y_train)\n",
    "y_pred = knn_model.predict_proba(x_test)\n",
    "Brighton_KNN_0_Rps = statistics.mean(RPS(y_pred, y_test))\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 20, metric = \"euclidean\")\n",
    "knn_model.fit(x1_train, y_train)\n",
    "y1_pred = knn_model.predict_proba(x1_test)\n",
    "Brighton_KNN_1_Rps = statistics.mean(RPS(y1_pred, y_test))\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 20, metric = \"euclidean\")\n",
    "knn_model.fit(x2_train, y_train)\n",
    "y2_pred = knn_model.predict_proba(x2_test)\n",
    "Brighton_KNN_2_Rps = statistics.mean(RPS(y2_pred, y_test))\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 20, metric = \"euclidean\")\n",
    "knn_model.fit(x3_train, y_train)\n",
    "y3_pred = knn_model.predict_proba(x3_test)\n",
    "Brighton_KNN_3_Rps = statistics.mean(RPS(y3_pred, y_test))\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 20, metric = \"euclidean\")\n",
    "knn_model.fit(x4_train, y_train)\n",
    "y4_pred = knn_model.predict_proba(x4_test)\n",
    "Brighton_KNN_4_Rps = statistics.mean(RPS(y4_pred, y_test))\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 20, metric = \"euclidean\")\n",
    "knn_model.fit(x5_train, y_train)\n",
    "y5_pred = knn_model.predict_proba(x5_test)\n",
    "Brighton_KNN_5_Rps = statistics.mean(RPS(y5_pred, y_test))\n",
    "Brighton_Data = {'Brighton':[Brighton_Logistic_0_Rps, Brighton_Logistic_1_Rps, Brighton_Logistic_2_Rps, Brighton_Logistic_3_Rps,\n",
    "                     Brighton_Logistic_4_Rps, Brighton_Logistic_5_Rps, Brighton_SVC_0_Rps, Brighton_SVC_1_Rps,\n",
    "                     Brighton_SVC_2_Rps, Brighton_SVC_3_Rps, Brighton_SVC_4_Rps, Brighton_SVC_5_Rps, Brighton_KNN_0_Rps,\n",
    "                     Brighton_KNN_1_Rps, Brighton_KNN_2_Rps, Brighton_KNN_3_Rps, Brighton_KNN_4_Rps, Brighton_KNN_5_Rps]} \n",
    "Brighton_Df = pd.DataFrame(Brighton_Data)\n",
    "Brighton_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Westham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.155093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.146826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.153616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.156252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.144557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.149257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.141114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.137646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.137262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.140248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.142975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.139586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.148423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.142692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.143077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.138981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.153558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.140365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Westham\n",
       "0   0.155093\n",
       "1   0.146826\n",
       "2   0.153616\n",
       "3   0.156252\n",
       "4   0.144557\n",
       "5   0.149257\n",
       "6   0.141114\n",
       "7   0.137646\n",
       "8   0.137262\n",
       "9   0.140248\n",
       "10  0.142975\n",
       "11  0.139586\n",
       "12  0.148423\n",
       "13  0.142692\n",
       "14  0.143077\n",
       "15  0.138981\n",
       "16  0.153558\n",
       "17  0.140365"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Westham = pd.read_excel('Rikudou.xlsx', sheet_name=\"West Ham\")\n",
    "y = Westham.iloc[:, 31].values\n",
    "x = Westham.iloc[:, 3:31].values\n",
    "x = np.delete(x, 22, axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)\n",
    "estimator = SVC(kernel=\"linear\", degree = 2)\n",
    "selector = RFE(estimator, 15, step=1)\n",
    "selector = selector.fit(x_train, y_train)\n",
    "a = selector.support_\n",
    "b = []\n",
    "i = 0\n",
    "while i < 27:\n",
    "    if a[i] == True:\n",
    "        b.append(i)\n",
    "    i += 1\n",
    "x1_train = x_train[:, b]\n",
    "x1_test = x_test[:, b]\n",
    "estimator = LogisticRegression()\n",
    "selector = RFE(estimator, 15, step=1)\n",
    "selector = selector.fit(x_train, y_train)\n",
    "a = selector.support_\n",
    "b = []\n",
    "i = 0\n",
    "while i < 27:\n",
    "    if a[i] == True:\n",
    "        b.append(i)\n",
    "    i += 1\n",
    "x2_train = x_train[:, b]\n",
    "x2_test = x_test[:, b]\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "estimator = ExtraTreesClassifier()\n",
    "selector = RFE(estimator, 15, step=1)\n",
    "selector = selector.fit(x_train, y_train)\n",
    "a = selector.support_\n",
    "b = []\n",
    "i = 0\n",
    "while i < 27:\n",
    "    if a[i] == True:\n",
    "        b.append(i)\n",
    "    i += 1\n",
    "x3_train = x_train[:, b]\n",
    "x3_test = x_test[:, b]\n",
    "fa = FactorAnalyzer()\n",
    "fa.fit(x_train, 23)\n",
    "a = fa.get_eigenvalues()\n",
    "a1 = a[1:2]\n",
    "a2 =  list(a1)\n",
    "a3 = a2[0]\n",
    "b = 0\n",
    "i = 0\n",
    "while i < 27:\n",
    "    if a3[i] >= 0:\n",
    "        b += 1\n",
    "    i += 1\n",
    "transformer = FactorAnalyzer(n_factors= b, rotation = None)\n",
    "x4_train = transformer.fit_transform(x_train)\n",
    "x4_test = transformer.transform(x_test)\n",
    "pca = PCA(n_components=20, svd_solver='arpack')\n",
    "pca.fit(x_train)\n",
    "a = pca.explained_variance_ratio_\n",
    "b = 0\n",
    "i = 0\n",
    "j = 0\n",
    "while i < 27:\n",
    "    if j <= 0.8 :\n",
    "        j = j + a[i]\n",
    "        b += 1\n",
    "    i += 1\n",
    "\n",
    "pca = PCA(n_components=b, svd_solver='arpack')\n",
    "x5_train = pca.fit_transform(x_train)\n",
    "x5_test = pca.transform(x_test)\n",
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict_proba(x_test)\n",
    "Westham_Logistic_0_Rps = statistics.mean(RPS(y_pred, y_test))\n",
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x1_train, y_train)\n",
    "y1_pred = model.predict_proba(x1_test)\n",
    "Westham_Logistic_1_Rps = statistics.mean(RPS(y1_pred, y_test))\n",
    "Westham_Logistic_1_Rps\n",
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x2_train, y_train)\n",
    "y2_pred = model.predict_proba(x2_test)\n",
    "Westham_Logistic_2_Rps = statistics.mean(RPS(y2_pred, y_test))\n",
    "Westham_Logistic_2_Rps\n",
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x3_train, y_train)\n",
    "y3_pred = model.predict_proba(x3_test)\n",
    "Westham_Logistic_3_Rps = statistics.mean(RPS(y3_pred, y_test))\n",
    "Westham_Logistic_3_Rps\n",
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x4_train, y_train)\n",
    "y4_pred = model.predict_proba(x4_test)\n",
    "Westham_Logistic_4_Rps = statistics.mean(RPS(y4_pred, y_test))\n",
    "Westham_Logistic_4_Rps\n",
    "model = LogisticRegression(solver = 'lbfgs')\n",
    "model.fit(x5_train, y_train)\n",
    "y5_pred = model.predict_proba(x5_test)\n",
    "Westham_Logistic_5_Rps = statistics.mean(RPS(y5_pred, y_test))\n",
    "Westham_Logistic_5_Rps\n",
    "param_grid = {'C':[1,10,100],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf', 'poly']}\n",
    "grid = GridSearchCV(SVC(),param_grid,refit = True, verbose=0)\n",
    "grid.fit(x_train,y_train)\n",
    "a = grid.best_params_\n",
    "model = SVC(C = 10, gamma = 0.1, kernel = 'rbf', probability = True)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict_proba(x_test)\n",
    "Westham_SVC_0_Rps = statistics.mean(RPS(y_pred, y_test))\n",
    "grid.fit(x1_train,y_train)\n",
    "a = grid.best_params_\n",
    "model = SVC(C = 10, gamma = 0.1, kernel = 'rbf', probability = True)\n",
    "model.fit(x1_train, y_train)\n",
    "y1_pred = model.predict_proba(x1_test)\n",
    "Westham_SVC_1_Rps = statistics.mean(RPS(y1_pred, y_test))\n",
    "grid.fit(x2_train,y_train)\n",
    "a = grid.best_params_\n",
    "model = SVC(C = 10, gamma = 0.1, kernel = 'rbf', probability = True)\n",
    "model.fit(x2_train, y_train)\n",
    "y2_pred = model.predict_proba(x2_test)\n",
    "Westham_SVC_2_Rps = statistics.mean(RPS(y2_pred, y_test))\n",
    "model = SVC(C = 10, gamma = 0.1, kernel = 'rbf', probability = True)\n",
    "model.fit(x3_train, y_train)\n",
    "y3_pred = model.predict_proba(x3_test)\n",
    "Westham_SVC_3_Rps = statistics.mean(RPS(y3_pred, y_test))\n",
    "model = SVC(C = 10, gamma = 0.1, kernel = 'rbf', probability = True)\n",
    "model.fit(x4_train, y_train)\n",
    "y4_pred = model.predict_proba(x4_test)\n",
    "Westham_SVC_4_Rps = statistics.mean(RPS(y4_pred, y_test))\n",
    "model = SVC(C = 10, gamma = 0.1, kernel = 'rbf', probability = True)\n",
    "model.fit(x5_train, y_train)\n",
    "y5_pred = model.predict_proba(x5_test)\n",
    "Westham_SVC_5_Rps = statistics.mean(RPS(y5_pred, y_test))\n",
    "param_grid = {'n_neighbors': np.arange(1, 20), 'metric': [\"euclidean\", \"manhattan\"] }\n",
    "knn = KNeighborsClassifier()\n",
    "a = knn_gscv.best_params_\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 20, metric = \"euclidean\")\n",
    "knn_model.fit(x_train, y_train)\n",
    "y_pred = knn_model.predict_proba(x_test)\n",
    "Westham_KNN_0_Rps = statistics.mean(RPS(y_pred, y_test))\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 20, metric = \"euclidean\")\n",
    "knn_model.fit(x1_train, y_train)\n",
    "y1_pred = knn_model.predict_proba(x1_test)\n",
    "Westham_KNN_1_Rps = statistics.mean(RPS(y1_pred, y_test))\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 20, metric = \"euclidean\")\n",
    "knn_model.fit(x2_train, y_train)\n",
    "y2_pred = knn_model.predict_proba(x2_test)\n",
    "Westham_KNN_2_Rps = statistics.mean(RPS(y2_pred, y_test))\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 20, metric = \"euclidean\")\n",
    "knn_model.fit(x3_train, y_train)\n",
    "y3_pred = knn_model.predict_proba(x3_test)\n",
    "Westham_KNN_3_Rps = statistics.mean(RPS(y3_pred, y_test))\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 20, metric = \"euclidean\")\n",
    "knn_model.fit(x4_train, y_train)\n",
    "y4_pred = knn_model.predict_proba(x4_test)\n",
    "Westham_KNN_4_Rps = statistics.mean(RPS(y4_pred, y_test))\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 20, metric = \"euclidean\")\n",
    "knn_model.fit(x5_train, y_train)\n",
    "y5_pred = knn_model.predict_proba(x5_test)\n",
    "Westham_KNN_5_Rps = statistics.mean(RPS(y5_pred, y_test))\n",
    "Westham_Data = {'Westham':[Westham_Logistic_0_Rps, Westham_Logistic_1_Rps, Westham_Logistic_2_Rps, Westham_Logistic_3_Rps,\n",
    "                     Westham_Logistic_4_Rps, Westham_Logistic_5_Rps, Westham_SVC_0_Rps, Westham_SVC_1_Rps,\n",
    "                     Westham_SVC_2_Rps, Westham_SVC_3_Rps, Westham_SVC_4_Rps, Westham_SVC_5_Rps, Westham_KNN_0_Rps,\n",
    "                     Westham_KNN_1_Rps, Westham_KNN_2_Rps, Westham_KNN_3_Rps, Westham_KNN_4_Rps, Westham_KNN_5_Rps]} \n",
    "Westham_Df = pd.DataFrame(Westham_Data)\n",
    "Westham_Df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we merge all of these columns to obtain final result table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = pd.concat([Liverpool_Df, Leicester_models, Mancity_models, Chelsea_models, Sheffield_models, Wolves_models, Tottenham_models,\n",
    "          Manu, Newcastle_models, Burnley, Crystal, Bourne, Everton,\n",
    "          Southampton, Norwich_models, Watford_models, Astonvilla_models, Arsenal_models, Brighton_Df,\n",
    "           Westham_Df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Liv</th>\n",
       "      <th>Lei</th>\n",
       "      <th>ManC</th>\n",
       "      <th>Chel</th>\n",
       "      <th>Shef</th>\n",
       "      <th>Wolv</th>\n",
       "      <th>Tot</th>\n",
       "      <th>Manu</th>\n",
       "      <th>New</th>\n",
       "      <th>Bur</th>\n",
       "      <th>Cry</th>\n",
       "      <th>Bou</th>\n",
       "      <th>Eve</th>\n",
       "      <th>Sou</th>\n",
       "      <th>Nor</th>\n",
       "      <th>Wat</th>\n",
       "      <th>Ast</th>\n",
       "      <th>Ars</th>\n",
       "      <th>Bri</th>\n",
       "      <th>Wes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Org. Log.</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RFES Log</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RFEL Log</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RFET Log</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>FAC Log</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PCA Log</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Org. SVC.</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RFES SVC</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RFEL SVC</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RFET SVC</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>FAC SVC</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PCA SVC</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Org. KNN.</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RFES KNN</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RFEL KNN</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RFET KNN</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>FAC KNN</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PCA KNN</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Liv   Lei  ManC  Chel  Shef  Wolv   Tot  Manu   New   Bur   Cry  \\\n",
       "Org. Log.  0.19  0.14  0.30  0.18  0.16  0.16  0.19  0.20  0.19  0.17  0.17   \n",
       "RFES Log   0.19  0.15  0.26  0.18  0.17  0.13  0.19  0.20  0.19  0.16  0.16   \n",
       "RFEL Log   0.19  0.15  0.29  0.18  0.20  0.12  0.19  0.20  0.19  0.16  0.16   \n",
       "RFET Log   0.18  0.15  0.30  0.18  0.20  0.14  0.19  0.20  0.19  0.16  0.15   \n",
       "FAC Log    0.10  0.11  0.12  0.14  0.24  0.10  0.16  0.16  0.16  0.17  0.15   \n",
       "PCA Log    0.19  0.14  0.25  0.16  0.20  0.11  0.17  0.19  0.18  0.16  0.15   \n",
       "Org. SVC.  0.13  0.13  0.19  0.13  0.07  0.09  0.16  0.16  0.16  0.16  0.13   \n",
       "RFES SVC   0.14  0.12  0.21  0.14  0.09  0.10  0.15  0.16  0.16  0.14  0.13   \n",
       "RFEL SVC   0.15  0.13  0.19  0.14  0.07  0.10  0.15  0.17  0.17  0.14  0.14   \n",
       "RFET SVC   0.14  0.13  0.17  0.14  0.06  0.11  0.14  0.17  0.16  0.15  0.13   \n",
       "FAC SVC    0.12  0.11  0.12  0.13  0.06  0.09  0.11  0.15  0.15  0.15  0.13   \n",
       "PCA SVC    0.13  0.13  0.19  0.13  0.07  0.09  0.15  0.17  0.16  0.14  0.14   \n",
       "Org. KNN.  0.17  0.13  0.24  0.16  0.09  0.11  0.15  0.20  0.18  0.16  0.15   \n",
       "RFES KNN   0.14  0.14  0.25  0.17  0.08  0.09  0.16  0.18  0.18  0.17  0.15   \n",
       "RFEL KNN   0.17  0.14  0.27  0.16  0.11  0.09  0.17  0.20  0.19  0.17  0.14   \n",
       "RFET KNN   0.17  0.14  0.25  0.17  0.11  0.11  0.17  0.21  0.18  0.16  0.15   \n",
       "FAC KNN    0.12  0.14  0.18  0.13  0.07  0.09  0.12  0.15  0.15  0.15  0.14   \n",
       "PCA KNN    0.16  0.12  0.23  0.16  0.10  0.10  0.16  0.20  0.17  0.15  0.14   \n",
       "\n",
       "            Bou   Eve   Sou   Nor   Wat   Ast   Ars   Bri   Wes  \n",
       "Org. Log.  0.19  0.18  0.19  0.26  0.22  0.23  0.19  0.25  0.16  \n",
       "RFES Log   0.18  0.18  0.18  0.24  0.22  0.23  0.19  0.25  0.15  \n",
       "RFEL Log   0.18  0.18  0.18  0.25  0.22  0.23  0.19  0.24  0.15  \n",
       "RFET Log   0.19  0.18  0.19  0.24  0.22  0.23  0.18  0.23  0.16  \n",
       "FAC Log    0.18  0.17  0.17  0.15  0.17  0.23  0.16  0.15  0.14  \n",
       "PCA Log    0.19  0.16  0.19  0.27  0.19  0.24  0.17  0.21  0.15  \n",
       "Org. SVC.  0.17  0.15  0.15  0.22  0.18  0.21  0.15  0.19  0.14  \n",
       "RFES SVC   0.17  0.17  0.17  0.22  0.19  0.21  0.16  0.19  0.14  \n",
       "RFEL SVC   0.17  0.17  0.17  0.23  0.18  0.20  0.16  0.21  0.14  \n",
       "RFET SVC   0.16  0.16  0.18  0.23  0.18  0.20  0.15  0.18  0.14  \n",
       "FAC SVC    0.16  0.14  0.14  0.23  0.17  0.20  0.12  0.17  0.14  \n",
       "PCA SVC    0.16  0.15  0.17  0.23  0.17  0.20  0.15  0.17  0.14  \n",
       "Org. KNN.  0.17  0.16  0.19  0.27  0.21  0.25  0.18  0.17  0.15  \n",
       "RFES KNN   0.18  0.17  0.17  0.23  0.19  0.24  0.18  0.20  0.14  \n",
       "RFEL KNN   0.19  0.17  0.18  0.26  0.21  0.23  0.18  0.18  0.14  \n",
       "RFET KNN   0.17  0.16  0.18  0.27  0.20  0.25  0.18  0.18  0.14  \n",
       "FAC KNN    0.14  0.13  0.17  0.27  0.20  0.21  0.13  0.14  0.15  \n",
       "PCA KNN    0.17  0.17  0.18  0.29  0.19  0.24  0.18  0.18  0.14  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results.columns = ['Liv', 'Lei', 'ManC', 'Chel', 'Shef', 'Wolv', 'Tot', 'Manu', 'New', 'Bur', 'Cry', 'Bou', 'Eve','Sou', 'Nor',\n",
    "                   'Wat', 'Ast', 'Ars', 'Bri', 'Wes']\n",
    "Results = Results.round(2)\n",
    "Results.index = (\"Org. Log.\", \"RFES Log\", \"RFEL Log\", \"RFET Log\", \"FAC Log\", \"PCA Log\",\n",
    "                  \"Org. SVC.\", \"RFES SVC\", \"RFEL SVC\", \"RFET SVC\", \"FAC SVC\", \"PCA SVC\",\n",
    "                  \"Org. KNN.\", \"RFES KNN\", \"RFEL KNN\", \"RFET KNN\", \"FAC KNN\", \"PCA KNN\")\n",
    "Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see the RPS results of each model for each dataset type. On team basis dataset types and methods are listed:;\n",
    "- Liverpool: Factor Analysis / Logistic Regression\n",
    "- Leicester: Factor Analysis / Logistic Regression\n",
    "- Manchester City: Factor Analysis / Logistic Regression - SVC\n",
    "- Chelsea: Factor Analysis / KNN\n",
    "- Sheffield: RFE Extra Trees - Factor Analysis / SVC\n",
    "- Wolverhampton: RFE Lo g - KNN\n",
    "- Tottenham: Factor Analysis - SVC\n",
    "- Manchester United: Factor Analysis - KNN\n",
    "- Newcastle: Factor Analysis - SVC\n",
    "- Burnley: RFE SVC - SVC\n",
    "- Crystal Palace: RFE Extra Trees - SVC\n",
    "- Bournemouth: Factor Analysis - KNN\n",
    "- Everton: Factor Analysis - KNN\n",
    "- Southampton: Original Dataset - SVC\n",
    "- Norwich: Factor Analysis - Logistic Regression\n",
    "- Watford: Factor Analysis - Logistic Regression\n",
    "- Aston Villa: RFE Logistic - SVC\n",
    "- Arsenal: Factor Analysis - SVC\n",
    "- Brighton: Factor Analysis - KNN\n",
    "- Westham: PCA - SVC\n",
    "\n",
    "As we see, generally the best feature engineering method for this dataset is Factor Analysis with pivoting to the lower bound eigenvalue of 0.8.\n",
    "\n",
    "\n",
    "Now, in order to measure our success even more, we will find the best model in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Liv     0.10\n",
       "Lei     0.11\n",
       "ManC    0.12\n",
       "Chel    0.13\n",
       "Shef    0.06\n",
       "Wolv    0.09\n",
       "Tot     0.11\n",
       "Manu    0.15\n",
       "New     0.15\n",
       "Bur     0.14\n",
       "Cry     0.13\n",
       "Bou     0.14\n",
       "Eve     0.13\n",
       "Sou     0.14\n",
       "Nor     0.15\n",
       "Wat     0.17\n",
       "Ast     0.20\n",
       "Ars     0.12\n",
       "Bri     0.14\n",
       "Wes     0.14\n",
       "dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Best_Models = Results.min(axis = 0)\n",
    "Best_Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, if we use the best model for each dataset, we obtain an RPS value between 0.6 and 0.2. Now\n",
    " we will calculate our mean RPS score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RPS Score is   0.13100000000000003\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "while i < 20:\n",
    "    j = j + Best_Models[i]\n",
    "    i += 1\n",
    "print(\"Mean RPS Score is\" , \" \" , j/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, we have reached 0.13 RPS score as the result of this study. Although the model took so much time to build, by benefiting the prepared \"baydogan(x,y) function, it is easier to use any team's dataset and build an accurate model. In this project, decision tree method is also deployed. Nevertheless, results were not accurate compared to these models, thus we excluded them in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Future Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our study, we used Premier League matches starting from 2004, and we created unique datasets for each of the teams. We normalized related rows and standardized all the columns. We tried to extract as much as relevant features as possible. After the preprocessing phase; by using 5 different feature engineering methods and eliminate / extract the features based on the mathematical grounds, we applied grid search on these 6 datasets, with K-NN and SVC. Additionally, we fit logistic regression to all of the datasets.\n",
    "\n",
    "There are possible implementations that could be applied in order to enhance the accuracy further. First of all, the study is conducted under a dataset which contains the information about match results of both teams in the previous consecutive games, the bets for these games and the team info. Nevertheless, other crucial information about the under / over results, goal numbers and teams’ previous non-league matches are not included in the dataset.  The reason behind this intuition is the fact that the goal numbers and several other parameters which are connected to the result of the matches can’t be known before a match. Thus, separate forecasting models must be built to predict these parameters, and the outcomes can be used in the model which predicts the match outcomes. \n",
    "\n",
    "Alternatively, different ensemble methods can be derived from the methodology applied in this study. For instance, we used team-based models to predict match outcomes. In this method,  output of the same match can be predicted by two different models; one for each team. Thus, combining the results for both teams for each match is a viable option.\n",
    "\n",
    "Lastly, as we mentioned in our results, our supposed methodology failed to provide grid search results for Brighton and Westham. Thus, several minor adjustments could be done to include these teams in our methodology."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
